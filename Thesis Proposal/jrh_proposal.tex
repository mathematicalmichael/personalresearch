\documentclass{amsart}
\usepackage{amssymb, amsmath, graphicx, caption, enumerate}
\graphicspath{ {images/} }
\usepackage{amsthm}
\usepackage{xargs}
\usepackage{scalerel}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\ds}{\displaystyle}
\newcommand{\op}[1]{\left(#1\right)}
\newcommand{\cp}[1]{\left[#1\right]}
\newcommand{\av}[1]{\left| #1\right|}
\newcommand{\st}[1]{\left\{#1\right\}}


\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\question}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\add}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improve}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}
\newcommandx{\remove}[2][1=]{\todo[linecolor=yelllow,backgroundcolor=yellow!10,bordercolor=red,#1]{#2}}


\newcommand\reallywidehat[1]{\arraycolsep=0pt\relax%
\begin{array}{c}
\stretchto{
  \scaleto{
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern-.5pt\bigwedge\kern-.5pt}
    {\rule[-\textheight/2]{1ex}{\textheight}} %WIDTH-LIMITED BIG WEDGE
  }{\textheight} % 
}{0.5ex}\\           % THIS SQUEEZES THE WEDGE TO 0.5ex HEIGHT
#1\\                 % THIS STACKS THE WEDGE ATOP THE ARGUMENT
\rule{-1ex}{0ex}
\end{array}
}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-3in}
\calclayout

\setcounter{secnumdepth}{0}
\usepackage{titlesec}
\titleformat{\section}{\center\large\bfseries}
\titleformat{\subsection}{\center}


% TOC
\setcounter{tocdepth}{1}


\begin{document}


\title{Thesis Proposal}

\author{Jordan R. Hall}


\maketitle

\begin{abstract}

Obtaining the solution to stochastic inverse problems is an important challenge in computational science. By solving a stochastic inverse problem, one may find an updated  characterization of parameter space that combines  prior beliefs and observable data. Using a new approach detailed in [Butler], we may solve for a probability distribution in parameter space, $\Lambda$, such that its push-forward (\emph{i.e.,} image) through the parameter-to-data map $f$ matches the observed probability density on the data, $\mathcal{D}$. Under certain conditions [Butler, Tarantola], an approximate solution of the stochastic inverse problem can be obtained as the solution of a regularized, convex minimization problem.
  
In this proposal, we consider a noisy map $f$ from a high-dimensional $\Lambda$ to $\mathcal{D}$ of specified dimension, where the gradient of $f$ exists, but may be inaccessible. If the gradient of $f$ is accessible, the minimization problem can be solved using gradient-based descent methods. If the gradient of $f$ is inaccessible, one may apply Derivative-Free (DF) optimization schemes as in [Chen and Wild]. In either case,  the effectiveness of the optimization algorithm may be enhanced by performing dimension reduction in $\Lambda$ as in [Russi, Constantine]. We consider different strategies for computing the active subspace of $f(\Lambda)$ in the context of both approaches, and apply them to the data-consistent inversion of two model problems. Both of these model problems are related to stochastic inverse problems arising in the simulation of plasmas in fusion reactors.
 

%\question{Do we know what the citation style is for this abstract?  Potentially check out the cite package}

 
\end{abstract}





\tableofcontents

\setcounter{tocdepth}{0}

\newpage
\section{Literature Review and Framework}

In this section we provide a literature review focused on inverse problem theory, derivative-free (DF) optimization, and dimension reduction. In the process, we will build a theoretical framework upon which to pose research questions, state initial results, and form a research plan.


\subsection{Data-Consistent Inversion}

\vspace{.125cm}

We begin our discussion of inverse problems by defining a parameter space $\Lambda$ of dimension $N$, a map or ``model" $f$, and a data space $\mathcal{D}$, which in our setting may be known values of $f(\cdot)$. In this proposal, we assume that $\mathcal{D}$ has dimension less than $N$, which is due to our assumption that evaluations of the model, $f$, are expensive and its inputs are many. In the following, we closely follow [Butler] to formulate an inverse problem. First, we assume prior knowledge of the parameter space, given by the so-called ``prior distribution," $\pi_\Lambda(\lambda)$. In practical applications, a prior distribution may be given by application experts, but may also be a state of knowledge on $\Lambda$ obtained by other mathematical or statistical processes. Second, we assume that we have a so-called ``observed" density, $\pi_\mathcal{D}$, which represents our state of knowledge of observed data, which is uncertain due to noise in $f$ and potential measurement error. Finally, we may form a ``push-forward," which is the density obtained by solving a \textit{forward problem}, $f\left(\pi_\Lambda \right)$. We denote the solution to the forward problem with $\pi_\Lambda^\mathcal{D}$.

\subsection{Optimization Methods for Solving Inverse Problems}

With our assumptions of a high-dimensional $\Lambda$ and expensive $f$, approximately solving the forward problem to obtain the push-forward density converges slowly, typically at Monte Carlo rates of $ \frac{1}{\sqrt{N}}$. As in [Tarantola], depending on $f$, the solution to the inverse problem can be obtained exactly or approximately obtained by solving an equivalent deterministic convex optimization problem. As in [Butler], we may alter the aforementioned classical formulation of the deterministic optimization problem to ensure we obtain, depending on $f$, the \textit{data-consistent} exact or approximate solution to the inverse problem.

We begin by examining the classical formulation of a deterministic optimization problem which corresponds to either exactly or approximately solving the stochastic inverse problem under consideration: for a linear map $f$, the exact solution is obtained; for nonlinear maps, we obtain an approximate solution. As in [Taratola], we define the \textit{classical misfit function}

\begin{equation} \label{eq:1}
S(\Lambda)=\frac{1}{2}\left(||f(\lambda)-d_{\text{obs}}||_\mathcal{D}^2+||\lambda-\lambda_{\text{prior}}||_\Lambda^2\right).
\end{equation}

\subsection{Derivative-Free Optimization}

\todo{Varis: I'll write an intro here connecting DF algorithms to noisy simulations due to turbulence/chaos to }

We first consider the Derivative Free (DF) algorithm suited for additive and multiplicative noise outlined by Chen and Wild\question{J: Put Citation Here}. This technique requires nothing more than evaluations of the noisy model and random draws from a normal distribution. Briefly, this method finds a new iterate by randomly perturbing the previous iterate in $\Lambda$; iterates are not allowed to stray much, though, due to relatively small smoothing factors and step sizes. The smoothing factor and step size in the DF algorithms are of great importance to their convergence and termination. As in [Chen and Wild], both the smoothing factor and step size will depend on a scale factor of the $L_1$ Lipschitz constant of $f$. As such, it will be of interest to obtain estimates of $L_1$, which is not straightforward in a gradient-free setting. We refer to [Others] for Lipschitz constant learning in this setting. 

\subsection{Dimension Reduction}

In our setting, it may be advantageous to perform dimension reduction on $f$. In particular, we shall consider Active Subspace methods described by Paul Constantine in \cite{Constantine} and an equivalent method by T.M. Russi in []. These techniques seek to explain outputs $f(x)$ in a subspace $A:=A(f;x)$ for which the $\dim (A) <N$. 

%\vspace{.25cm}

\noindent Assuming that we lack the analytic $\nabla f$, one initializes the method by performing $M$ random draws of $x_i \in \Lambda$. We then compute $f(x_i)$ for all $i=1,\ldots,M$ samples, which we note will require, at the very least, $M$ evaluations of $f$; in a realistic setting, this would require $M$ solves of a model (e.g., $M$ solves of a PDE constrained system). In our notations, we have $\mathcal{D}=\{f(x_i)\}_{i=1}^M$. Next, we need $\nabla_\Lambda f$ evaluated at $x_i$ for all $i=1,\ldots,M$, which we assume that we do not have in closed analytic form. Hence, we generally need some gradient approximation method, and typically a locally linear approximation to the gradient is a fair balance between reasonably estimating the gradient and not pushing computational expenses to an unreasonable regime. With this approximation formed, we denote each estimation to $\nabla f(x_i)$ with $\reallywidehat{\nabla f}(x_i)$ and we define the $M \times n$ matrix
\begin{eqnarray}
W:=\begin{bmatrix}
\reallywidehat{\nabla f}(x_1)^T\\
\cdot \cdot \cdot\\
\reallywidehat{\nabla f}(x_i)^T\\
\cdot \cdot \cdot \\
\reallywidehat{\nabla f}(x_M)^T\\
\end{bmatrix},
\end{eqnarray} which defines a covariance structure of $f$ over $\Lambda$. This interpretation of (1) leads us to the idea of computing the Singular Value Decomposition of $W$,

\begin{eqnarray}
W=U\Sigma V^*,
\end{eqnarray} where $U$ is $M \times M$ unitary, $\Sigma$ is $M \times n$ diagonal with the singular values of $W$ along its diagonal, and $V^*$ is $n \times n$ unitary. With the singular values of $W$ in hand, we search for a drop-off in the spectrum of $W$. In detail, we plot the singular values, $\{\sigma_i\}_{i=1}^n$ and seek a drop-off in magnitude between some pair of singular values, $\sigma_{j}$ and $\sigma_{j+1}$. The active subspace is the span of $u_1,\ldots,u_{j}$, which are the first $j$ columns of $W$, the so-called ``left singular vectors" of $W$. We let $A\left(f;\mathcal{D}\right):=\text{span}\{u_1,\ldots,u_{j}\}$ to denote the active subspace of $f$ with respect to the data $\mathcal{D}$. We choose to use a notation with $\mathcal{D}$ included to emphasize the dependence of the active subspace on the random draws made in $\Lambda$, which led to our particular $\mathcal{D}$.

\vspace{.25cm}

\noindent The fact that $u_1,\ldots,u_{j}$ correspond to the nontrivial singular values is exactly why they account for the most amount of variance in function values. In fact, one can view AS analysis as an artful choice of principal components after a \textit{full} Principal Component Analysis (PCA) is performed in the gradient space $W$; for more details on this viewpoint, we refer the interested reader to Section 6.4 in T.M. Russi \cite{Russi}.

\vspace{.25cm}

\noindent For a point $\Tilde{x} \in \Lambda$, we may write $\ds \sum_{i=1}^{j-1}\left( u_i^T \Tilde{x}\right)u_i \in A(f;\mathcal{D})$, which is a projection of the point $\Tilde{x}$ in the ``active directions" of $f$. We call this projection an ``active variable," which is a point in the active subspace $A$, which we will use to abbreviate $A(f;\mathcal{D})$ when it is clear to which $f$ and data $\mathcal{D}$ we are referring.

\vspace{.25cm}

We have arrived at the property that 

\begin{eqnarray}
f\left(\sum_{i=1}^{j-1} \left(u_i^T x\right) u_i\right) \approx f(x).
\end{eqnarray} 

In practice, we can check the extent to which the active subspace accounts for functions values $f(x)$ by checking for resolution in a so-called ``sufficient summary plot" \cite{Constantine}, where we plot active variables against function values. In these plots, we hope to see a pattern between the active variables versus the actual function values. For example, if $f$ is quadratic in its active variables, then we expect to see quadratically-resolved sufficient summary plots. We will revisit sufficient summary plots in the Methods and Algorithms section. For now, we continue describing the AS minimization method with the understanding that sufficient summary plots allow us to check the ``fit" of the active subspace.

\subsection{Application: Magnetic Equilibria in Tokamaks}
\todo{Varis: I'll write this, covering Grad-Shafonov Equations, and why these MHD
equilibria are important to kinetic simulations.}



\section{Research Questions}










%%%%%%%%%%%%%% References %%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{65}

\bibitem{Constantine} Constantine, Paul G. ``Active Subspaces: Emerging Ideas for Dimension Reduction in Parameter Studies." SIAM, 2015. 


The Active Subspaces software library and interactive Jupyter notebooks can be found at \texttt{https://github.com/paulcon/active\_subspaces}.

\bibitem{Chen and Wild} Chen and Wild. ``Randomized Derivative-Free Optimization of Noisy Convex Functions." Funded by the Department of Energy. 2015.

\bibitem{Russi} Russi, Trent M. ``Uncertainty Quantification with Experimental Data and Complex System Models." Dissertation, University of California Berkeley. 2010.

\bibitem{Smith}  Smith, Ralph.``Uncertainty Quantification: Theory, Implementation, and Applications.â€ SIAM, 2013.



 



\end{thebibliography}


\end{document}

