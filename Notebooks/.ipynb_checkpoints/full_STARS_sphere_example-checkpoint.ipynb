{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Noisy Sphere Function\n",
    "\n",
    "Let $f: R^N \\to R$ equal the sphere function; i.e., $$f(x; \\xi)=\\sum_{i=1}^N x_i^2 + \\epsilon (\\xi),$$ where $\\epsilon \\sim U[-k,k]$ is stoachastic additive noise with zero mean and bounded variance. Then we have $\\nabla f(x)=2x$ and one can show that $L_1^*=2$. This means that the value of $||\\nabla f(x) -\\nabla f(y)||$ will never be larger than twice the value of $2||x-y||$ which is obvious because $||\\nabla f(x) -\\nabla f(y)||$ literally equals $2||x-y||$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy sphere function evaluated at x_test is 8.008579884812791 and the error-free value is 8\n"
     ]
    }
   ],
   "source": [
    "# Define sphere function with additive noise\n",
    "\n",
    "# Noise level\n",
    "k=1e-2\n",
    "\n",
    "def sphere(x):\n",
    "    x=x**2\n",
    "    return (np.sum(x, axis=0) + k*(2*np.random.rand(1) - 1))[0]\n",
    "\n",
    "F=sphere # rename function as F due to other code\n",
    "\n",
    "N=4 # this is the dimension of the inputs!\n",
    "\n",
    "x_test=np.sqrt(2)*np.ones(N)\n",
    "\n",
    "print('Noisy sphere function evaluated at x_test is', F(x_test), 'and the error-free value is', 2*N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Estimate the standard deviation of the noise, $\\sigma_\\epsilon^2$\n",
    "\n",
    "Since $\\epsilon(\\cdot) \\sim U[-k,k]$, the variance in the rv $\\epsilon(\\cdot)$  is $\\sigma_\\epsilon ^2 = k^2/3$. The below implementation of EC Noise (Chen & More) will form estimators to $\\sigma_\\epsilon^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True variance is 3.3333333333333335e-05\n"
     ]
    }
   ],
   "source": [
    "# What's the right answer?\n",
    "\n",
    "correct_var=k**2/3\n",
    "\n",
    "print('True variance is', correct_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize difference table which is M x M\n",
    "# M is number of points to be sampled along some curve and evaluated under f(.)\n",
    "\n",
    "M=6\n",
    "\n",
    "T = np.zeros((M,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a base point:\n",
    "# ie: the first point on the curve\n",
    "x_b = np.ones((N,1))\n",
    "\n",
    "# Choose a direction to sample in, and normalize it:\n",
    "p_u = np.ones((N,1))\n",
    "p = p_u/np.linalg.norm(p_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form difference table T\n",
    "\n",
    "for i in range(0,M):\n",
    "    T[i,0] = F(x_b + (i/M)*p)\n",
    "\n",
    "for j in range(0,M-1):\n",
    "    for i in range(0,M-j-1):\n",
    "        T[i,j+1] = T[i+1,j] - T[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a row vector to store the k-level estimators (sigma_k^2) \n",
    "# ie: Initialize empty vector for storage, row vector for readability\n",
    "\n",
    "S = np.zeros((1,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build S according to paper; each k-th component of S is the k-th level estimator for the variance in our noise\n",
    "# which is computed using a scaled average of the k-th level difference values, from the difference table T\n",
    "\n",
    "for i in range(1,M):\n",
    "    S[0,i] = ((np.math.factorial(i)**2.)/np.math.factorial(2*i))*(1./(M-i))*np.sum(T[:,i]**2,axis=0)\n",
    "\n",
    "S=S[:,1:] # Don't need the first column (because first col. of T just holds function values!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.25819113e-01 5.37581940e-04 3.17486984e-05 3.29177844e-05\n",
      "  3.38672123e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Print our estimators\n",
    "\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did we do?\n",
    "# ie: compute a vector of relative errors between correct variance and k-th level estimator\n",
    "\n",
    "E = np.zeros((1,M-1))\n",
    "for i in range(0,M-1):\n",
    "    E[0,i] = (1./correct_var)*abs(S[0,i] - correct_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3.291778437310853e-05\n"
     ]
    }
   ],
   "source": [
    "# Print the level, k, of the estimator with smallest relative error, and print the error\n",
    "key_index=np.argmin(E)\n",
    "print(key_index, S[0,key_index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.291778437310853e-05\n",
      "[1.9706739e-05]\n"
     ]
    }
   ],
   "source": [
    "# Define our estimated variance as the best one\n",
    "\n",
    "est_var= S[0,key_index]\n",
    "print(est_var)\n",
    "\n",
    "another_estimator=np.sum(S[:,2:], axis=1)/(M-1)\n",
    "print(another_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Learn the $L_1$ Lipschitz constant\n",
    "\n",
    "We first compute a value $\\epsilon^*$ prescribed in a Callies paper which is a rough estimate of the the maximum contribution of error. For us, 3*(standard deviation) in the error will give an upper bound on 99.7% of error draws, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15804180953741195"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std=est_var**0.5\n",
    "eps_star=3*std\n",
    "eps_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a heuristic estimate for $||f''(x_0)||$ and it can be a very rough upper bound -- Chen and More show this experimentally. The following is directly adapted from Algorithm 5.1 in More and Wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7243162554310951\n"
     ]
    }
   ],
   "source": [
    "# Suggested parameters for an inequality we will test\n",
    "tao_1=100\n",
    "tao_2=0.1\n",
    "\n",
    "# We need a random draw\n",
    "x_0=10*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "\n",
    "# Define an arbitrary unit vector\n",
    "unit_v=np.ones(N)/(N**(1/N))\n",
    "\n",
    "# Computes a centered finite difference to approximate f''\n",
    "# Stores all 3 function evaluations\n",
    "def Delta(h):\n",
    "    F_m=F(x_0 - h*unit_v)\n",
    "    F_0=F(x_0)\n",
    "    F_p=F(x_0 + h*unit_v)\n",
    "    return np.array((abs(F_p - 2*F_0 + F_m), F_m, F_0, F_p))\n",
    "\n",
    "# Name some of the things we just made\n",
    "h_a=std**0.25\n",
    "DD=Delta(h_a)\n",
    "D_h_a=DD[0]\n",
    "F_m_a=DD[1]\n",
    "F_0_a=DD[2]\n",
    "F_p_a=DD[3]\n",
    "\n",
    "# Our first candidate for ||f''||\n",
    "mu_a=D_h_a/(N*(h_a)**2)\n",
    "\n",
    "# The LHS and RHS from two inequalities we need to test\n",
    "LHS_1=abs(F_p_a-F_0_a)\n",
    "LHS_2=abs(F_m_a-F_0_a)\n",
    "RHS_1=tao_2*max(abs(F_0_a),abs(F_p_a))\n",
    "RHS_2=tao_2*max(abs(F_0_a),abs(F_m_a))\n",
    "\n",
    "# We will use our first guess mu_a, unless some inequalities fail\n",
    "# then we will use mu_b, an alternate guess scaled by mu_a\n",
    "if D_h_a/std>=tao_1 and LHS_1<=RHS_1 and LHS_2<=RHS_2:\n",
    "    mu_f2=mu_a\n",
    "else:\n",
    "    h_b=(std/mu_a)**0.25\n",
    "    mu_f2=Delta(h_b)[0]/h_b**2\n",
    "    \n",
    "print(mu_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need estimates for the gradients at sample points, which are denoted $\\hat{\\nabla f (x_k)}$. We do so by evaluating the forward difference in each $i$-th component of a sample $x$ to approximate the $i$-th partial derivative of $f$ evaluated at $x$, $$\\frac{\\partial f}{\\partial x_i}(x)\\approx \\frac{f(x+ h^* \\cdot e_i)-f(x)}{h^*} \\quad h^*:=8^{1/4}\\left(\\frac{\\sigma}{\\mu_{f''}}\\right)^{1/2} \\quad \\mu_{f''} \\approx \\max |f''| \\quad e_i:=(0,\\ldots,0,1,0,\\ldots,0).$$ In our case, $\\mu_{f''}=2$ since the Hessian of $f$, $\\nabla^2 f$ is a $10 \\times 10$ diagonal matrix with entries of 2 along the diagonal, and we are using the standard Euclidean norm. Note that the Frobenius norm equals $\\sqrt{40}$.\n",
    "\n",
    "Chen and More show that $h^*$ yields the best estimates to the partial derivates in an $\\mathcal{L}_1$ sense.\n",
    "\n",
    "With a pairs of estimates $||\\nabla f(x_k)||, ||\\nabla f(x_{k+1})||$ we form the ratios $$\\frac{||\\nabla f(x_k)-\\nabla f(x_{k+1})||-2\\epsilon^*}{x_k-x_{k+1}}$$ and take the maximum such ratio as a numerical estimate to $L_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=6\n",
    "\n",
    "x_vals=np.zeros((N,1)) # matrix to store the vectors we randomly make\n",
    "\n",
    "grads=np.zeros((N,1)) # matrix to store the gradients we approximate\n",
    "\n",
    "ratios=np.zeros((M,1)) # vector storing the ratios we are intersted in!\n",
    "\n",
    "h_star=(8**0.25)*np.sqrt(std/mu_f2) # the prescribed step size by Chen/More\n",
    "\n",
    "for j in range(0,M):\n",
    "\n",
    "    # Take two draws inside a large hypercube and store them\n",
    "    x=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "    y=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "    x_vals=np.hstack((x_vals,x))\n",
    "    x_vals=np.hstack((x_vals,y))\n",
    "\n",
    "    # For loop to make finite diff approx's \n",
    "    # to each partial and form approx gradient at x\n",
    "    approx_grad_x=np.zeros((N,1))\n",
    "    for i in range(0,N):\n",
    "        e = np.zeros((N,1))\n",
    "        e[i] = 1.0\n",
    "        approx_grad_x[i] = (F(x + h_star*e) - F(x))/h_star\n",
    "    \n",
    "    # Store the gradient at x\n",
    "    grads=np.hstack((grads,approx_grad_x))\n",
    "    \n",
    "    # For loop to make finite diff approx's \n",
    "    # to each partial and form approx gradient at y\n",
    "    approx_grad_y=np.zeros((N,1))\n",
    "    for p in range(0,N):\n",
    "        e = np.zeros((N,1))\n",
    "        e[p] = 1.0\n",
    "        approx_grad_y[p] = (F(y + h_star*e) - F(y))/h_star\n",
    "    \n",
    "    # Store the gradient at y\n",
    "    grads=np.hstack((grads,approx_grad_y))\n",
    "    \n",
    "    # Form ratios to estimate L_1\n",
    "    # Note we subtract by 2*eps_star, which was mentioned in Callies paper\n",
    "    diff_1=np.linalg.norm(approx_grad_x - approx_grad_y) - 2*eps_star\n",
    "    diff_2=np.linalg.norm(x-y)\n",
    "    r=diff_1/diff_2\n",
    "    \n",
    "    ratios[j]=r\n",
    "    \n",
    "x_vals=x_vals[:,1:]\n",
    "grads=grads[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratios are [[1.9995732  1.99944471 1.99859251 1.99981603 2.00025085 2.00032343]] and our estimate to L_1 is 2.0003234304143405\n"
     ]
    }
   ],
   "source": [
    "L_1_est=(np.max(ratios))\n",
    "\n",
    "print('the ratios are', np.transpose(ratios),'and our estimate to L_1 is', L_1_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have now obtained estimates to both $\\sigma_\\epsilon^2$ and $L_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Perform STARs (with no bells and whistles)\n",
    "\n",
    "Chen and Wild define the optimal hyperparameters $$\\mu^* := \\left(\\frac{8\\sigma_\\epsilon^2 N}{L_1^2(N+6)^3}\\right)^{1/4} \\quad \\quad h:= \\frac{1}{4L_1(N+4),}$$ where $\\mu^*$ as a smoothing factor and $h$ is a step size. We will use our estimates to $\\sigma_\\epsilon^2$ and $L_1$ to compute the hyperparameters needed for STARs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03545269349519118 0.0023144405345842814\n"
     ]
    }
   ],
   "source": [
    "mu_star=((8*est_var*N)/(L_1_est**2*(N+6)**3))**0.25\n",
    "h=1/(4*L_1_est*(N+4))\n",
    "\n",
    "print(mu_star,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STARS(x_init,F, mu_star,h):\n",
    "    \n",
    "    # x_init: initial x value\n",
    "    # F: function we wish to minimize\n",
    "    # mu_star: smoothing parameter\n",
    "    # h: step length\n",
    "    \n",
    "    # Evaluate noisy F(x_init)\n",
    "    f = F(x_init)\n",
    "    \n",
    "    # Draw a random vector of same size as x_init\n",
    "    u = np.random.normal(0,1,(N,1))\n",
    "    \n",
    "    # Form vector y, which is a random walk away from x_init\n",
    "    y = x_init + (mu_star)*u\n",
    "    \n",
    "    # Evaluate noisy F(y)\n",
    "    g = F(y)\n",
    "    \n",
    "    # Form finite-difference \"gradient oracle\"\n",
    "    s = ((g - f)/mu_star)*u \n",
    "    \n",
    "    # Take descent step in direction of -s smooth by h to get next iterate, x_1\n",
    "    x = x_init - (h)*s\n",
    "   \n",
    "    # Evaluate noisy F(x_1)\n",
    "    f=F(x)\n",
    "    \n",
    "        \n",
    "    return [x, f, y, g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1) 173552.4769955103\n"
     ]
    }
   ],
   "source": [
    "# Draw a single x_init to seed all experiments with same value below\n",
    "\n",
    "x_init=100*(2*np.random.rand(N,1)-np.ones((N,1)))\n",
    "\n",
    "print(np.shape(x_init),F(x_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5775704523266336 0.3520123987610504\n"
     ]
    }
   ],
   "source": [
    "# Run pure STARs\n",
    "\n",
    "x=x_init\n",
    "f=F(x)\n",
    "\n",
    "x_hist=np.array(x)\n",
    "f_hist=np.array(f)\n",
    "\n",
    "maxit=5000\n",
    "\n",
    "for i in range(1, maxit):\n",
    "    s=STARS(x,F,mu_star,h)\n",
    "    x=s[0]\n",
    "    f=s[1]\n",
    "    x_hist=np.hstack((x_hist,x))\n",
    "    f_hist=np.hstack((f_hist,f))\n",
    "    \n",
    "print(np.linalg.norm(x),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt8VPWd//HXZyaZEAgEIeFiEgRMALkU0IitReul2tAWda2tot3dtlRqt7bb7m5d++vur7qutd1f96KtraXVsm5bKbXWikVpa6tUF5WgqFwVECEgEO53cpnP74+EEEICSWYm58zM+/l45JGZ75zLx2PIO9/zPed7zN0REZHsEwm6ABERCYYCQEQkSykARESylAJARCRLKQBERLKUAkBEJEspAEREspQCQEQkSykARESylAJARCRL5QRdwKkUFRX58OHDgy5DRCRtLF26dIe7F3dm2VAHwPDhw6murg66DBGRtGFm73R22VCeAjKz6WY2e+/evUGXIiKSsUIZAO4+391nFRYWBl2KiEjGCmUAiIhI6ikARESylAJARCRLhTIANAgsIpJ6FuZHQo6fONnnPfVct9aNRCAnEiEWjZATNXKiRm6k6XVuNEJuNEI0YkmuWEQkWGa21N0rO7NsqO8DeGv7AT58359Ttn0z2oSCkdPO+9yokRONkBMxYjlN33OiEWI5EfKOfc9p+h7LiRCLRsnLbQqfY215rZbJjUaImDV/QSTS/L2lzTCDaOT4MmbW/J6TPrfmtuixdSPH31vLdpuWN1PoiUiTUAfAWQN6c98nz+vWunF36hvjNDQ2fa+POw3H3sfj1Dc4DfE49Y1N7Scv49Q3xI8v0/z9wNGGlm3WNcapa2j6Otr8va4xTmM8vL0q4HjYnBQ8x8Lo5PZTBVA0Ygwt7EXZgN4MLexFfiyH3rlResei5MeixKKRE7fbzj6iEWNwvzz6944FfXhEskaoA6Bffi5V44cEXUaXNbQTDq0Dor4xTjzuxB3cm743uhN3b3ofb3p/7LO4O41xx5tfx53m9Y9/Hndv2WbL++bX7tDYevlWr71528eX9eZaOKGWY9s5uRanriFOze7DLF63k4N1jQkdu96xKLGcSFNvJtLUi4lGjn9FDPJjUQrzcynMz6VvXi798nPo2yuXMUP68v7yIvrkhfrHWiQ09C8lBXKiEXKiEbLtj1l352BdI4fqGjhc18ih5q/6xvhJQdQ6TOIODfE4W/ceYcueIzTE4y3LNcadhnhTuDU6NMbjHK5rZO/herbuPcL+Iw3sP9LA4fqm4IlFI4wZ2pch/XpR0CuH/vkxrj23hPEluqlQpK0eCwAzuwS4C1gBzHX3Z3tq39IzzIyCvBwKAvgL/GhDI0s37OZPa7azZtsBNuw8yMGjjew8eJSfvvQO10w6kykjBlI1fkgg9YmEUUL/EszsIeCjwHZ3H9+qvQq4F4gCP3b3bwEOHAB6ATWJ7FekrbycKBeWF3FhedEJ7bsO1nHHEyv43cptzKuu4auPvsbgvr2oGFxAQV4OZQN687eXV+i0kWSlhC4DNbOLafql/vCxADCzKPAmcAVNv+iXADOA1e4eN7PBwH+4+02n235lZaVrNlBJhnjceXXTHp5bs523dx5i065D7DtSz/rag/SJRSk5I5/rzx/Gpy8cTkSXB0sa67HLQN19kZkNb9M8BVjr7uubi5kLXO3uK5s/3w3kJbJfka6KRIzzzjqD884644T2l9/exZOvb2H1u/u568mVzF60jn69colGjIrBfTl/+BmMGtyXUYP7MqBPlg3qSMZLRb+3BNjU6n0NcIGZXQt8COgPfK+jlc1sFjALYNiwYSkoT+S4KSMGMGXEANyd3yzbwrNrtlPXGOdofZw/rtrG/Ne2tCzbJxYlGjFGFhfwD1eOZmpF0Sm2LBJ+qQiA9vrP7u6PAY+dbmV3n21m7wLTY7FY924CEOkiM+OaySVcM7mkpc3dqdl9mLd3HGT11n1s23eUuoY4f1y9nU8++BKD++VxRu8YJf3zmXXxSCaUFtI7prEESR+p+GmtAcpavS8FtnSwbLvcfT4wv7Ky8uZkFibSFWZG2YDelA3ozcWjjj9h7+8O1vGzl95h067D7Dx4lBfX7+KZ1dsBGFHUh4mlhbx35EA+Xlmm6UYk1BKeC6h5DODJVoPAOTQNAl8ObKZpEPhGd1/RhW1OB6aXl5ff/NZbbyVUn0iqbd9/hGfX1PLuniO8sXkPr27cw86DdVQMKuCaySWMHdqPqRVF5EZDOfeiZJiuDAInehXQI8AlQBGwDfiGuz9oZh8G/oumy0Afcve7u7N9XQUk6ernL23kR39ez9s7DgJNdzhXDh/ApLL+XDl2MCOK+ujSU0mJHguAVFEPQDLFviP1PLemlhfX7+SVjXtYvXUf7k3zMV0zuYSLKoq4emKJLj2VpEn7ADhGPQDJNO/sPMiyTXuo3rCbn7+8sWXiwI++ZyhXTTyTCaWFDC3MD7hKSWdpHwDqAUg2aIw7j71Sw/Nrd/DU8q3UNcQB+PsrRvHFyysCrk7SVdoHwDHqAUi2OFLfyJINu3h48Ts8s2obI4sLGFrYi/JBBcy6eKR6BdJpGfNAGJFs0Ss3ykUVxUws68/3/7SODTsOsmHnQf781g7++383MLGsPzOmDOPiimKGFPYKulzJEKHsAegUkEiT5Zv38qtXanhi2RZ2HqwDYFJZf/7+ylFcVFF8mrUlG+kUkEiGqWuIs/LdffxuxVa+/+w68nIi/OCT53LZmMFBlyYh05UA0J0pImkglhNhUll/bqsaw8IvXwzAZ+ZU8++/W8POA0cDrk7SVSgDwMymm9nsvXv3Bl2KSOiMHtKXRbddyhVjB/PdP67lfd/6I1/5xTIFgXSZTgGJpCl357WavfxiyUYeeXkTZQPyue1DY3h/eZGmrs5iugpIJAuYGZPK+jOprD/jSwr59lOr+eIjrxKNGA988jyuGKvxATk19QBEMkRj3HmtZg9/O/dVtuw5wtTyIr557QRK+usegmyS9oPAGgMQ6bpoxDh32Bn8dOYF3HTBMJ57s5YP3/tnXlq/M+jSJKTUAxDJUMs27eHmh6uJGPzwLyuZWFqImSady3Rp3wMQkcRNKuvPv398IoeONnLN/S9w1fde4HBdY9BlSYgoAEQy2MWjivnjP1zC9Iln8sbmvXzup0tbZiAVUQCIZLjivnl85+PvYWRxHxa9Wcvn/mcpG5ofVCPZLZQBoEFgkeTKy4ny1N9exE0XDOMPq7ZxyXee5a4nV/JGjf6NZTMNAotkmeWb93LPU6v433U7yY1GeOCT53Lp6EEaIM4QGgQWkQ6NLynkZ599L3/4uw/QNy+Hz8yp5iu/WKYB4iykABDJUmcXF/DC7ZfxuQ+M5PFlW/jcT5cS5jMCknwKAJEs1is3ytemncMnKktZ9GYtv1iyKeiSpAcpAESEe659DxNLC7n9sTe468mVQZcjPaRHA8DM+pjZUjP7aE/uV0ROLRoxHvzU+VSNG8KDz7/N/yzeEHRJ0gMSCgAze8jMtpvZ8jbtVWa2xszWmtntrT76R2BeIvsUkdQoKsjjvhmTGTW4gH/+zQpdIpoFEu0BzAGqWjeYWRS4H5gGjAVmmNlYM/sgsBLYluA+RSRFYjkR7r1hMgBf+PkrrN2+P+CKJJUSCgB3XwTsatM8BVjr7uvdvQ6YC1wNXAq8F7gRuNnMNP4gEkLnDO3Hg39dyf4j9Uz/7gs8vXxr0CVJiqTil3AJ0PpSghqgxN2/7u5fBn4O/Mjd4+2tbGazzKzazKpra2tTUJ6InM7l5wzml7e8j7zcCA88ty7ociRFUhEA7d1O2HJxsbvPcfcnO1rZ3WcDdwKvxGJ6rJ1IUMoH9eWzU0ewbNMebnv0NY7U60axTJOKAKgBylq9LwW2dGUD7j7f3WcVFhYmtTAR6ZqZU0dy3XmlzKuu4ctzlwVdjiRZKgJgCVBhZiPMLAbcADzRlQ1oMjiRcMiPRfnOxyfy+UvO5ukVW1n6TtshP0lniV4G+giwGBhtZjVmNtPdG4BbgYXAKmCeu6/oynbVAxAJl1s+cDa9ciPc/dtVmi4ig+QksrK7z+igfQGwoLvbNbPpwPTy8vLubkJEkqgwP5dp44fy61c3s2LLPsaX6I+zTBDKSzHVAxAJny9cWk4sJ8KX5r7KnkN1QZcjSRDKANAYgEj4lA8q4OHPTKFm12Fufria+sZ2r+SWNBLKAFAPQCSc3jtyIN+4aixLNuzmq798TSGQ5kIZAOoBiITXjVOG8ZEJQ3l82RYee6Um6HIkAaEMAPUARMLLzPjejZMZO7Qf9zy1WpPGpbFQBoCIhJuZ8d0bJ1PfEOcWPUksbYUyAHQKSCT8zi4u4I6rxrF5z2Hu/9PaoMuRbghlAOgUkEh6+Ni5pVx49kDufeYtDhxtCLoc6aJQBoCIpIdIxLj54pHUNzrT7l2kU0FpRgEgIgm5dPQg/up9Z7Fp12F+uVRXBaWTUAaAxgBE0ss3po9j2IDezFuy6fQLS2iEMgA0BiCSXqIRY9r4IVS/s5vfrdATxNJFKANARNLPV64YxeB+efzjr15n76H6oMuRTlAAiEhS9MqN8k8fGcvuQ/X8x+/XBF2OdIICQESSZvrEM7lszCDmLtlE9QY9PCbsFAAiklR3XjWOvJwIX5m3jMN1eo5wmIUyAHQVkEj6KhvQm3tnTGbTrsPcOb9LDwOUHhbKANBVQCLp7dLRg5g2fghzl2xi2aY9QZcjHQhlAIhI+rvn2gkUFeTxnYUaEA4rBYCIpET/3jGuHDeY59fu0FhASCkARCRlLiovAuDfFq4OuBJpjwJARFJm2oShTC0v4icvbCAe10RxYdNjAWBm55jZA2b2qJl9vqf2KyLBumR0MQA/e3ljwJVIWwkFgJk9ZGbbzWx5m/YqM1tjZmvN7HYAd1/l7rcAnwAqE9mviKSPmVNHMLKoD//v6dUc1DMDQiXRHsAcoKp1g5lFgfuBacBYYIaZjW3+7CrgeeCZBPcrImnCzLjrmvHsO9LAv/52lZ4ZECIJBYC7LwLa3u89BVjr7uvdvQ6YC1zdvPwT7n4hcFNH2zSzWWZWbWbVtbW1iZQnIiHx/vIirq8s45GXN/L4ss1BlyPNUjEGUAK0nhS8Bigxs0vM7D4z+yGwoKOV3X22u1e6e2VxcXEKyhORINz9F+OZWNafby5YrQHhkEhFAFg7be7uz7r7l9z9c+5+/yk3oKkgRDJOTjTCpy48i9r9R1m6cXfQ5QipCYAaoKzV+1JgS1c2oKkgRDLTB88ZDMBL63cGXIlAagJgCVBhZiPMLAbcADzRlQ2oByCSmfr2yuXMwl786M9vc6hOVwQFLdHLQB8BFgOjzazGzGa6ewNwK7AQWAXMc3dNCSgiANxWNYa9h+v5yQsbgi4l61mYL8mqrKz06urqoMsQkSS76ccv8nbtQRbddik5UU1IkExmttTdO3WvlY68iPS4my44iy17j/CynhoWqFAGgMYARDLbhWcPBODVjXpWQJBCGQC6Ckgks/XvHWPs0H78+M/radQ9AYEJZQCoByCS+T570Qh2H6rndyu2Bl1K1gplAKgHIJL5po0fSn5ulNt+9ToNjfGgy8lKoQwAEcl8+bEo/+fDY9h/pIHVW/cHXU5WCmUA6BSQSHaYWtE039evX9UEcUEIZQDoFJBIdhhR1IfxJf14evlWnQYKQCgDQESyx00XnMXmPYdZ+o4miOtpCgARCdT0iWdSmJ/LP/9mOZv3HA66nKwSygDQGIBI9ijIy+Fb107gzW0H+GX1ptOvIEkTygDQGIBIdpk2YSgTSwt5evlWPSymB4UyAEQk+1xXWcbqrft5Y7N6/j1FASAioXDZmEEA/OqVmoAryR4KABEJhZL++VwyupifvvhO0KVkDQWAiITGqMF9iTts2nUo6FKyQigDQFcBiWSn688vIzdqfPePbwVdSlYIZQDoKiCR7HR2cQHXnVfKvOoa1m4/EHQ5GS+UASAi2ev684cBsOjN2oAryXwKABEJlfFn9gNg7pKNAVeS+RQAIhIqOdEIN14wjDe3HeA3yzRLaCr1WACY2TVm9iMz+42ZXdlT+xWR9HPH9HGUDcjn+39ah7vuDE6VhALAzB4ys+1mtrxNe5WZrTGztWZ2O4C7P+7uNwOfAq5PZL8iktliORGuO7eMNdt0Z3AqJdoDmANUtW4wsyhwPzANGAvMMLOxrRb5p+bPRUQ69Mn3Ng0GP/aKTgOlSkIB4O6LgF1tmqcAa919vbvXAXOBq63Jt4Gn3P2VRPYrIplvYEEeV4wdzCMvb2T7/iNBl5ORUjEGUAK0ntO1prnti8AHgevM7JaOVjazWWZWbWbVtbW6DEwkm33+krM52hBn/mvvBl1KRkpFAFg7be7u97n7ee5+i7s/0NHK7j7b3SvdvbK4uDgF5YlIuphY2h+AF9fvDLiSzJSKAKgBylq9LwW2dGUDmgpCRACiEeNj55by+5XbqN7Q9myzJCoVAbAEqDCzEWYWA24AnkjBfkQkC3zp8nIAHnhuXcCVZJ5ELwN9BFgMjDazGjOb6e4NwK3AQmAVMM/dV3Rlu5oLSESOOWtgH268YBgvrt9FQ2M86HIySqJXAc1w96Hunuvupe7+YHP7Ancf5e5nu/vdXd2uTgGJSGvvGzmQA0cb+PWruiQ0mUI5FYR6ACLSWtX4IeRGTfcEJFkoA0A9ABFpLTca4QOjilm8fqemhkiiUAaAegAi0tbQwnwAnl+7I+BKMkcoA0BEpK2vVo1mcL88/vXJVUGXkjFCGQA6BSQibfXrlcvU8mLWbNvP7oN1QZeTEUIZADoFJCLtufycQQCsq9XjIpMhlAEgItKe0UP6AvD2joMBV5IZQhkAOgUkIu0pPaNpIHjLHs0OmgyhDACdAhKR9uTlRCkqyGPT7kNBl5IRQhkAIiIdGTYgn0eX1uh+gCRQAIhIWpla0TRN/Lt7dRooUaEMAI0BiEhHLh3dFADzX+vSLPPSjlAGgMYARKQjk4edQUn/fJa+szvoUtJeKANARORUzhnaj7e2616ARCkARCTtFBXEeHvHQeoa9HyARCgARCTtlA3oDcDuQ5oSIhGhDAANAovIqYws6gPAk6+/G3Al6S2UAaBBYBE5lXFnNv1u+PlL7wRcSXoLZQCIiJzKsIG9+URlKetqD/KCng/QbQoAEUlLN0wZBsDvV24LuJL0pQAQkbQ0qbQ/AFv2HA64kvSlABCRtBSJGCOL+7BZAdBtPRYAZjbSzB40s0d7ap8iktn+YlIJK9/dxy49IaxbEgoAM3vIzLab2fI27VVmtsbM1prZ7QDuvt7dZyayPxGR1t5fUYQ7LF63M+hS0lKiPYA5QFXrBjOLAvcD04CxwAwzG5vgfkRETvKekkL65uXwvK4E6paEAsDdFwG72jRPAdY2/8VfB8wFrk5kPyIi7cmJRrhg5ECeX1sbdClpKRVjACXAplbva4ASMxtoZg8Ak83sax2tbGazzKzazKpra/U/VURO7b0jB7Bp12Fq9JSwLktFAFg7be7uO939Fnc/293v6Whld58N3Am8EovFUlCeiGSSycOaLgf9ZXVNwJWkn1QEQA1Q1up9KdClJzdoKggR6axzhvYD4N5n3gq4kvSTigBYAlSY2QgziwE3AE90ZQOaDE5EOqt3LIe+eTkAHK5rDLia9JLoZaCPAIuB0WZWY2Yz3b0BuBVYCKwC5rn7iq5sVz0AEemKb1/3HgBefFuXg3ZFTiIru/uMDtoXAAu6u10zmw5MLy8v7+4mRCSLnF1cAMCnf7KEDd/6SMDVpI9QTgWhHoCIdMXoIX1bXuspYZ0XygDQGICIdNU1k84E4GM/+N+AK0kfoQwA9QBEpKu+dHkFAG9s3qvB4E4KZQCoByAiXTWyeRwA4Jz/+3SAlaSPUAaAegAi0h2r/qXq9AtJi1AGgIhId+THokGXkFYUACKSkfYdqQ+6hNALZQBoDEBEuqu4bx4AP160PuBKwi+UAaAxABHprr+/YhSAnhHQCaEMABGR7vp4ZdNclIX5uQFXEn4KABHJKNFI04z0f1qj54mcTigDQGMAIpIM8bgHXUKohTIANAYgIsmwrGZP0CWEWigDQEQkET/+q0oAXt2oADgVBYCIZJzzhw8A4L9+/2bAlYSbAkBEMk6//KZHnew/2hBwJeEWygDQILCIJMLMWl7vOHA0wErCLZQBoEFgEUmWq7/3QtAlhFYoA0BEJFG3Xtr0SNm/fN9ZAVcSXgoAEclIN14wDIBYVL/mOqIjIyIZ6YzeMQD+5cmVAVcSXgoAEclIejbA6fVYAJhZHzP7bzP7kZnd1FP7FRF5XXcEtyuhADCzh8xsu5ktb9NeZWZrzGytmd3e3Hwt8Ki73wxclch+RUS64psLVgVdQigl2gOYA5zwEE4ziwL3A9OAscAMMxsLlAKbmhdrTHC/IiKn9dUPjQbgxfW7Aq4knBIKAHdfBLQ9slOAte6+3t3rgLnA1UANTSGQ8H5FRDpjUln/lteNmhn0JKn4RVzC8b/0oekXfwnwGPAxM/sBML+jlc1slplVm1l1ba3m8xaR7htYEGt5vXLLvgArCadUBIC10+buftDdP+3un3f3n3W0srvPBu4EXonFYh0tJiJyWmOG9Gt5Pf17zwdYSTilIgBqgLJW70uBLV3ZgKaCEJFU0ANiTpSKAFgCVJjZCDOLATcAT3RlA5oMTkSS5a9bTQVxzfc1L1BriV4G+giwGBhtZjVmNtPdG4BbgYXAKmCeu6/oynbVAxCRZLlqUknL69dr9EdlazmJrOzuMzpoXwAs6O52zWw6ML28vLy7mxARAWByqyuB5EShvBxTPQARSZZIpL3rUgRCGgAaAxCRVBl++2+p3a+HxEBIA0A9ABFJpruuHnfC+/Pv/kO3t7V9/xH+sHJbp5bde7ge9+NXHrk7//rkStZs3d/t/SdTKANAPQARSaZrJpec1Db89t8y/Pbftrz/7evvcuV/PtfyC/snL7zN9T9cfMI6Dzy3jil3P8NnH66moTHe0r593xHmLWm6/3XPoTpWbNnL5j2HmXjn7xjxtePDoe/uPcKPn3+bD/3XIqDpstR9R04MiZ6U0CBwqrj7fGB+ZWXlzUHXIiLp71TTQEy4YyHfmD6Of/jlawDUHjjK0fo4d85veo5A65BorfUmp3zzGQDKBxfw5bnL2LjrED/8y/NaPv/P379J9Tu7iB/PjJO2u+FbHwHgUF0Dew/XM7Qwv/P/gd1kQSVPZ1RWVnp1dXXQZYhImmtojFP+9aeSus3Vd1XRKzfKgaMNjP/GwoS3d1vVaG6+aCQVzXUeC4SuMrOl7l7ZmWV1CkhEMl5ONEJRQV5Stzn35Y0ASfnlD/BvT6/hOwvXJGVbnRXKANAgsIgk221Vo5O6vTvmr+RvfrY0qdv84aL1La/rW40xpEooA0BEJNkmlCT/D8oFb2xN+jZ7kgJARLLCOUP78cYdVwZdRqdFLPU3sIUyADQGICKp0LdXLsMH9g66jE7piRuYQxkAGgMQkVS5rWpM0CV0imVrD0BEJFWuHDs46BJCQwEgIlklJxphw7c+woN/3alL5TOaAkBEslJn7oHt7s1Y6UIBICJZ6aJRRaf8/H9mTkl5DZPK+jO71ZQRPS2UAaCrgEQk1fJyoqz6l6qW9w996vgpoYpBBVxUUZyU/VxUcXLQrPvmh1lx54d4/Avv58pxQ5Kyn+4IZQDoKiAR6Qn5sSjzb53KP390LJeNOT44PLAg1vL6sb+58IR1ft3mfXtW33U8WO68ahwTSgr5+WcvAOAnnz6faMTok9fxXJyLvnppp/8bEhHK2UBFRHrKhNJCJpSe+Mfm1PLjf7WfO+yME8YCGjqYouHrHz6HuxesAk68iWtkcQHzvzgV6PyYQtmA1M8ECiHtAYiIBGFAn6a//D9xflmHy+REIzz95YtOar/54pF86sLhPP6F9xPLSexXa0/cAwDqAYiItHjhHy/jzW37GdS31ymXGzOkHwu/fHHLg12OueOq408e+8KlZ7Njf12n9nv5mEE8s3p71wtOUI8FgJmNBL4OFLr7dT21XxGRzsqPRZlY1r9Ty44e0rfl9Vc+OOqkz7/6oc7fcXxm/5455dNWp/opZvaQmW03s+Vt2qvMbI2ZrTWz20+1DXdf7+4zEylWRCSMvnhZeULrO+F+JOQc4HvAw8cazCwK3A9cAdQAS8zsCSAK3NNm/c+4e8/3b0REekAkwZnbciLBDMd2KgDcfZGZDW/TPAVY6+7rAcxsLnC1u98DfDSZRYqIZLKvXDEKM/jJCxt6dL+JjAGUAJtava8BLuhoYTMbCNwNTDazrzUHRXvLzQJmAQwbNiyB8kREUusvJpcw7sx+CW+nMD+Xb0wfx8fOLaX2wNEkVNY5iQRAe32eDk9kuftO4JbTbdTdZwOzoemh8N2uTkQkxf7z+klJ3d74FDy17FQSOfFUA7S+WLYU2JJYOU00FYSISOolEgBLgAozG2FmMeAG4InklCUiIqnW2ctAHwEWA6PNrMbMZrp7A3ArsBBYBcxz9xXJKEpzAYmIpF5nrwKa0UH7AmBBUiui6RQQML28PLFra0VEpGOhnAtIPQARkdQLZQBoEFhEJPVCGQDqAYiIpF4oA0BERFIvlNNBHxsEBvaZ2VtAIdD2fFDbtrbvi4Adqayzg7qSud7pljvV5505Zu21ZeJx7MyyHX3elfZTHcswH8eurKufyeSsl8rjeFYn9t/E3UP/Bcw+XVs776uDqCuZ651uuVN93pljli3HMZFj2ZX2Ux3LMB/Hrqyrn8n0OY6d+UqXU0DzO9HW3jKp1t19dna90y13qs87c8zaa8vE49iZZTv6vCvtQR/LRPann8nk7DNMx/G0rDk9Mo6ZVbt7ZdB1pDsdx+TQcUweHcvkSZceQHfMDrqADKHjmBw6jsmjY5kkGdsDEBGRU8vkHoCIiJyCAkBEJEspAEREslRWBICZ9TGz/zazH5nZTUHXk87MbKSZPWhmjwZdSzozs2uafx5/Y2ZXBl1PujKzc8zsATN71Mw+H3Q96SZtA8DMHjKz7Wa2vE17lZmtMbO1ZnZ7c/O1wKMZGtDnAAABz0lEQVTufjNwVY8XG3JdOZbuvt7dZwZTabh18Tg+3vzz+Cng+gDKDa0uHsdV7n4L8AlAl4Z2UdoGADAHqGrdYGZR4H5gGjAWmGFmY2l6XOWxB9g39mCN6WIOnT+W0rE5dP04/lPz53LcHLpwHM3sKuB54JmeLTP9pW0AuPsiYFeb5inA2ua/UuuAucDVND2/uLR5mbT9b06VLh5L6UBXjqM1+TbwlLu/0tO1hllXfx7d/Ql3vxDQ6d0uyrRfhiUc/0sfmn7xlwCPAR8zsx8QzG3l6ajdY2lmA83sAWCymX0tmNLSSkc/k18EPghcZ2a3BFFYmuno5/ESM7vPzH5ICp5OmOlCORtoAqydNnf3g8Cne7qYNNfRsdwJ6BdW53V0HO8D7uvpYtJYR8fxWeDZni0lc2RaD6AGKGv1vhTYElAt6U7HMjl0HJNDxzEFMi0AlgAVZjbCzGLADcATAdeUrnQsk0PHMTl0HFMgbQPAzB4BFgOjzazGzGa6ewNwK7AQWAXMc/cVQdaZDnQsk0PHMTl0HHuOJoMTEclSadsDEBGRxCgARESylAJARCRLKQBERLKUAkBEJEspAEREspQCQEQkSykARESylAJARCRL/X/fWNSSisUiRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(f_hist)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
