{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the $L_1$ Lipschitz constant of a function\n",
    "\n",
    "In many problems of interest in optimization and other fields, target functions $f: R^N \\to R$ must have bounded slopes, in the sense that for any $x,y \\in R^N$ $\\exists L_1 >0$ so that\n",
    "\n",
    "$$||\\nabla f(x) - \\nabla f(y)|| \\leq L_1 ||x-y||,$$\n",
    "\n",
    "where $||\\cdot||$ denotes the usual Euclidean norm in $R^N$; i.e., $||x||^2=\\sum_{i=1}^N x_i^2, x\\in R^N$. We can call the Euclidean norm by np.linalg.norm(). Such a constant $L_1$ is called the Liptschitz constant of $f$, and we will write $L_1^*$ to denote the supremum or least upper bound of all possible values $L_1$ for which the inequality in question may hold. We need to determine $L_1^*$ to find out what the tightest bound in variations of $\\nabla f(\\cdot)$ are. \n",
    "\n",
    "In this notebook, we numerically find a value for an estimator $\\hat{L_1^*}$ using approximations to derivatives in our noisy, black-box setting using a paper and results from More and Wild. Their main result provides an optimal value $h$ in a forward difference approximation to the directional derivative $f', p \\in R^N$ with $$f' \\approx \\frac{f(x+hp)-f(x)}{h}.$$ In fact, More and Wild show that with certain assumptions, using their optimum $h$ value, this approximation is the best, in the sense of minimizing a norm in the function space $\\mathcal{L}_1$; importantly the authors assume the presence of additive noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "Let $f: R^N \\to R$ equal the sphere function; i.e., $$f(x; \\xi)=\\sum_{i=1}^N x_i^2 + \\epsilon (\\xi),$$ where $\\epsilon \\sim U[-k,k]$ is stoachastic additive noise with zero mean and bounded variance. Then we have $\\nabla f(x)=2x$ and one can show that $L_1^*=2$. This means that the value of $||\\nabla f(x) -\\nabla f(y)||$ will never be larger than twice the value of $2||x-y||$ which is obvious because $||\\nabla f(x) -\\nabla f(y)||$ literally equals $2||x-y||$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199.99999046]\n"
     ]
    }
   ],
   "source": [
    "k=1e-5\n",
    "\n",
    "def sphere(x):\n",
    "    return np.sum(x**2, axis=0) + k*(2*np.random.rand(1) - 1)\n",
    "\n",
    "F=sphere\n",
    "\n",
    "print(F(np.array((10,10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytically check $L_1$ (just to make sure we aren't doing anything crazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301.66208860006196\n"
     ]
    }
   ],
   "source": [
    "# Draw two random points in a large hypercube:\n",
    "\n",
    "N=10\n",
    "\n",
    "x_0=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "y_0=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "\n",
    "d_1=np.linalg.norm(x_0-y_0)\n",
    "print(d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603.3241772001239\n"
     ]
    }
   ],
   "source": [
    "# Now compute the difference in their derivatives\n",
    "d_2=np.linalg.norm(2*(x_0-y_0))\n",
    "\n",
    "print(d_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(d_2/d_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Approach\n",
    "\n",
    "We will compute ratios at samples $x_k, x_j \\in R^N$ $$\\frac{||\\hat{\\nabla f(x_j)}-\\hat{\\nabla f(x_k)}||- 2 \\epsilon^*}{||x_j - x_k||},j \\neq k$$  where $\\hat{\\nabla f (x_k)}$ denotes a numerical approximation to $\\nabla f(x_k)$. Here $\\epsilon^* := \\sup_{\\xi} \\epsilon (\\xi),$ a technique mentioned in a Callies paper as a slightly-better-than-ad-hoc way of estimating $L_1$: slightly better since we are trying to remove the average effect of the noise. In our example, we have $\\epsilon^* \\approx \\bar{\\epsilon} + 3\\sigma$, where $\\bar{\\epsilon}$ is the mean of the noise and $\\sigma^2$ is the variance of the noise. We estimate $\\sigma$ numerically in another notebook, but here we know the mean and variance of our noise. The mean is 0 and variance is $k^2/3$. Adding three times the standard deviation to the mean is a slightly optimistic upper bound on the average contribution of noise, but should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688777e-05"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std=k/np.sqrt(3)\n",
    "eps_star=3*std\n",
    "eps_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a small heuristic estimate for $||f''(x_0)||.$ The following is directly adapted from Algorithm 5.1 in More and Wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030305457428767113"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tao_1=100\n",
    "tao_2=0.1\n",
    "\n",
    "x_0=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "\n",
    "unit_v=np.ones(N)/(N**(1/N))\n",
    "\n",
    "def Delta(h):\n",
    "    F_m=F(x_0 - h*unit_v)\n",
    "    F_0=F(x_0)\n",
    "    F_p=F(x_0 + h*unit_v)\n",
    "    return np.array((abs(F_m - 2*F_0 + F_p), F_m, F_0, F_p))\n",
    "\n",
    "h_a=std**0.25\n",
    "DD=Delta(h_a)\n",
    "D_h_a=DD[0][0]\n",
    "F_m_a=DD[1][0]\n",
    "F_0_a=DD[2][0]\n",
    "F_p_a=DD[3][0]\n",
    "\n",
    "D_h_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.612499362410437\n"
     ]
    }
   ],
   "source": [
    "mu_a=D_h_a/h_a**2\n",
    "\n",
    "LHS_1=abs(F_p_a-F_0_a)\n",
    "LHS_2=abs(F_m_a-F_0_a)\n",
    "\n",
    "RHS_1=tao_2*max(abs(F_0_a),abs(F_p_a))\n",
    "RHS_2=tao_2*max(abs(F_0_a),abs(F_m_a))\n",
    "\n",
    "if D_h_a/std>=tao_1 and LHS_1<=RHS_1 and LHS_2<=RHS_2:\n",
    "    mu_f2=mu_a\n",
    "else:\n",
    "    h_b=(std/mu_a)**0.25\n",
    "    mu_f2=Delta(H_b)[0]/h_b**2\n",
    "    \n",
    "print(mu_f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need estimates for the gradients at sample points, which are denoted $\\hat{\\nabla f (x_k)}$. We do so by evaluating the forward difference for in $i$-th component of a sample $x$ to approximate the $i$-th partial derivate of $f$ at x, $$\\frac{\\partial f}{\\partial x_i}(x)\\approx \\frac{f(x+ h^* \\cdot e_i)-f(x)}{h^*} \\quad h^*:=8^{1/4}\\left(\\frac{\\sigma}{\\mu_{f''}}\\right)^{1/2} \\quad \\mu_{f''} \\approx \\max |f''| \\quad e_i:=(0,\\ldots,0,1,0,\\ldots,0).$$ In our case, $\\mu_{f''}=2$ since the Hessian of $f$, $\\nabla^2 f$ is a $10 \\times 10$ diagonal matrix with entries of 2 along the diagonal, and we are using the standard Euclidean norm. Note that the Frobenius norm equals $\\sqrt{40}$.\n",
    "\n",
    "Chen and More show that $h^*$ yields the best estimates to the partial derivates in an $\\mathcal{L}_1$ sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=6\n",
    "\n",
    "x_vals=np.zeros((N,1)) # matrix of the vectors we randomly make\n",
    "\n",
    "grads=np.zeros((N,1)) # matrix of the gradients we approximates\n",
    "\n",
    "ratios=np.zeros((M,1)) # vector storing the ratios we are intersted in!\n",
    "\n",
    "#mu_f2=2\n",
    "h_star=(8**0.25)*np.sqrt(std/mu_f2)\n",
    "\n",
    "F=sphere\n",
    "\n",
    "for j in range(0,M):\n",
    "\n",
    "    x=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "    y=100*(2*np.random.rand(N,1) - np.ones((N,1)))\n",
    "    x_vals=np.hstack((x_vals,x))\n",
    "    x_vals=np.hstack((x_vals,y))\n",
    "\n",
    "    approx_grad_x=np.zeros((N,1))\n",
    "    for i in range(0,N):\n",
    "        e = np.zeros((N,1))\n",
    "        e[i] = 1.0\n",
    "        approx_grad_x[i] = (F(x + h_star*e) - F(x))/h_star\n",
    "    \n",
    "    grads=np.hstack((grads,approx_grad_x))\n",
    "    \n",
    "    approx_grad_y=np.zeros((N,1))\n",
    "    for p in range(0,N):\n",
    "        e = np.zeros((N,1))\n",
    "        e[p] = 1.0\n",
    "        approx_grad_y[p] = (F(y + h_star*e) - F(y))/h_star\n",
    "    \n",
    "    grads=np.hstack((grads,approx_grad_y))\n",
    "    \n",
    "    diff_1=np.linalg.norm(approx_grad_x - approx_grad_y) - 2*eps_star\n",
    "    diff_2=np.linalg.norm(x-y)\n",
    "    r=diff_1/diff_2\n",
    "    \n",
    "    ratios[j]=r\n",
    "    \n",
    "x_vals=x_vals[:,1:]\n",
    "grads=grads[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00000551]\n"
     ]
    }
   ],
   "source": [
    "average=np.sum(ratios,axis=0)/M\n",
    "\n",
    "print(average)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
