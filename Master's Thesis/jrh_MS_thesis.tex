\documentclass{amsart}
\usepackage{amssymb, amsmath, graphicx, caption, enumerate}
\graphicspath{ {images/} }
\usepackage{amsthm}
\usepackage{xargs}
\usepackage{scalerel}
\usepackage[ ]{algorithm2e}
\usepackage[dvipsnames]{xcolor}

\makeatletter
\renewcommand{\@algocf@capt@plain}{above}% formerly {bottom}
\makeatother


\newcommand{\D}{\mathcal{D}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\mustar}{\mu^*}
\newcommand{\ds}{\displaystyle}
\newcommand{\op}[1]{\left(#1\right)}
\newcommand{\cp}[1]{\left[#1\right]}
\newcommand{\av}[1]{\left| #1\right|}
\newcommand{\st}[1]{\left\{#1\right\}}




\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\question}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\add}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improve}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}
\newcommandx{\remove}[2][1=]{\todo[linecolor=yelllow,backgroundcolor=yellow!10,bordercolor=red,#1]{#2}}


\newcommand\reallywidehat[1]{\arraycolsep=0pt\relax%
\begin{array}{c}
\stretchto{
  \scaleto{
    \scalerel*[\widthof{\ensuremath{#1}}]{\kern-.5pt\bigwedge\kern-.5pt}
    {\rule[-\textheight/2]{1ex}{\textheight}} %WIDTH-LIMITED BIG WEDGE
  }{\textheight} % 
}{0.5ex}\\           % THIS SQUEEZES THE WEDGE TO 0.5ex HEIGHT
#1\\                 % THIS STACKS THE WEDGE ATOP THE ARGUMENT
\rule{-1ex}{0ex}
\end{array}
}

\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-2.75in}
\calclayout

\setcounter{secnumdepth}{0}
\usepackage{titlesec}
\titleformat{\section}{\Large\bfseries}
\titleformat{\subsection}{}






\begin{document}





\begin{abstract}

  
We consider maps $f$ with additive or multiplicative noise from a high-dimensional parameter space $\Lambda$ to a data space $\mathcal{D}$ of lower dimension, where $\nabla f$ exists, but may be inaccessible. Many problems in Uncertainty Quantification require minimizing such an $f$. We investigate Derivative-Free Optimization (DFO) in this setting. The considered DFO algorithm's hyperparameters depend on two generally unknown parameters -- the $L_1$ Lipschitz constant of $f$ and the bounded variance in the noise of $f$, $\sigma^2$. We firstly explore efficiently learning $L_1$ and $\sigma^2$ from samples. To learn $L_1$, approximations to $\nabla^2 f$ are needed. 
Secondly, we examine methods of enhancing the DFO algorithm by performing dimension reduction in $\Lambda$.  
The DFO hyperparmeters are inversely proportional to the known dimension $P$ of $\Lambda$, resulting in heavy smoothing and small step sizes for large $P$.
By learning a lower-dimensional \textit{active subspace} $\mathcal{A}$ of $f$ which defines directions causing the majority of the variance of $f$, iterates in a DFO algorithm may be updated with steps only taken in $\mathcal{A}$, reducing the value of $f$ more efficiently than updating iterates in the full variables, $\Lambda$. 
Dimension reduction can be performed with access to samples  and does require approximations to $\nabla f$. In addition to computational savings made by stepping only in active directions when an active subspace exists, computational costs may be reduced further by learning hyperparameters and the active subspace from DFO iterates whenever possible, reducing map evaluations.

 

\end{abstract}


\title{Optimizing Noisy, High-Dimensional Functions}

\author{Jordan R. Hall}

\maketitle


% TOC
\setcounter{tocdepth}{1} 
\tableofcontents






\newpage


\section{Introduction}


We begin our discussion by defining a parameter space $\Lambda$ of dimension $P$, a map or ``model" $f$, and a data space $\mathcal{D}:=f(\Lambda)$. We assume that $\mathcal{D}$ has dimension $D$, typically with $D<< P$. Points in $\mathcal{D}$ may be known values of $f(\lambda), \lambda\in \Lambda$; as such, we may write $d=f(\lambda)$ to denote the particular datum corresponding to the evaluation of a point $\lambda \in \Lambda$ under $f$. Points in $\mathcal{D}$ may also be \textit{observed} data, denoted $d_{\text{obs}}$, where the corresponding $\lambda\in \Lambda$ may be unknown. We assume realizations of $f$ are  noisy. We model observations of $f$ with $d_{\text{obs}}=\hat{f}(\lambda;\xi)=f(\lambda)+\epsilon(\xi)$, which is an additive noise structure, or $d_{\text{obs}}=\hat{f}(\lambda;\xi)=f(\lambda)(1+\epsilon(\xi))$, which is a multiplicative noise structure. In both cases $\epsilon(\cdot)$ denotes a random variable specified by realizations or draws $\epsilon(\xi)$. We assume $\epsilon(\cdot)$ has bounded variance $\sigma^2<\infty$.


Many objective functions formed in Uncertainty Quantification (UQ) applications generally fall into the category of maps $f$ described above. For example, one may solve a \textit{Stochastic Inverse Problem} (\textit{SIP}) by posing an equivalent deterministic optimization problem, with the heavy assumptions that $f$ is a linear map and the \textit{prior distribution} in $\Lambda$ and \textit{observed distribution} in $\mathcal{D}$ are Gaussian. Optimizing $f$ in the setting described above is a problem of \textit{Optimization Under Uncertainty} (\textit{OUU}).

In this section we provide a brief literature review and discussion of Derivative-Free Optimization (DFO), hyperparameter learning, and dimension reduction. We introduce a theoretical framework to unify results in the literature so that we may propose methods and state results in the proceeding sections.




\subsection{Derivative-Free Optimization}

Many important physical systems possess turbulent or chaotic behavior.  The physical state of the system $u(x,\lambda)$ and the corresponding parameter
to observable map $f(u(x,\lambda))$ may be modeled as a stochastic process, or as a deterministic function with additive or multiplicative noise.  
In this setting, the efficient extraction of accurate gradients of $f$ in parameter space is a challenging undertaking, as popular techniques based on
linearization, including adjoint methods, are inaccurate \cite{lea2000, Qiqi2014}.  
The finite-difference approximations to $\nabla f_\Lambda$ 
involve $P=\text{dim}(\Lambda)$ 
additional, usually nonlinear model solves for the physical system state $u(x,\lambda_i + \delta \lambda_i)$, and is greatly polluted by the noise in $f$.

As a consequence of these difficulties, optimization in this setting will need to be performed by algorithms which
do not require approximating gradient information from samples while online; otherwise, we expect to surpass a reasonable computational budget since even one gradient approximation will be $\mathcal{O}(P)$ (using, for instance, finite differencing.)
We consider \textit{Derivative-Free Optimization} (DFO) algorithms suited for additive and multiplicative noise. DFO algorithms require nothing more than the ability to evaluate the noisy model $f$ and randomly draw from a normal distribution; $\nabla f$ is unneeded once inputs and algorithm hyperparameters have been prescribed.

Many DFO algorithms find subsequent iterates $\lambda^{(k)}$ by random walks or \textit{ball walks} in $\Lambda$ specified by draws from Gaussian distributions in every coordinate of $\lambda^{(k)}$; iterates are not allowed to stray much, though, due to prescribed smoothing factors and step sizes. The smoothing factor and step size in DFO algorithms are of great importance to their convergence. Both the smoothing factor and step size depend on the $L_1$ Lipschitz constant of $f$. As such, it will be of interest to obtain estimates of $L_1$, which is not straightforward in a gradient-free setting. We refer to \cite{Calliess, KS} for Lipschitz constant learning in this setting, and discuss this problem more in the proceeding subsection.

As in \cite{CW}, we consider the additive noise OUU problem

\begin{eqnarray} \label{eq:1}
\min_{\lambda \in \Lambda} \quad \mathbb{E}\left[f(\lambda)+\epsilon(\xi)\right],
\end{eqnarray} 

\noindent and the multiplicative noise OUU problem

\begin{eqnarray} \label{eq:2}
\min_{\lambda \in \Lambda} \quad \mathbb{E}\left[f(\lambda)(1+\epsilon(\xi))\right],
\end{eqnarray} 

\noindent where the authors assume:

\begin{enumerate}[(i.)]

\item $f: \Lambda=\R^P \to \R$ is convex;

\item $\epsilon(\cdot)$ is a random variable with probability density $\pi_{\epsilon}(\epsilon(\xi))$;

\item for all $\lambda$ the noise $\epsilon(\cdot)$ is independent and identically distributed, has bounded variance $\sigma^2$, and is unbiased; i.e., $\mathbb{E}_\xi (\epsilon(\xi))=0$.



\end{enumerate}


In \cite{CW} the \textit{STARS (STep-size Approximation in Randomized Search)} DFO algorithm is used to solve the additive and multiplicative OUU problems with assumptions (i.)-(iii.) above. Briefly, STARS uses small perturbations of iterates in $\Lambda$ by the addition of a random vector with components drawn from a normal distribution, computes the noisy function value at the randomly perturbed point, and updates iterates using a Gaussian-smoothed finite-difference for approximate gradient information in a gradient-descent-type scheme. STARS only  requires the ability to evaluate $f$ and access to random draws from a normal distribution. All in all, the algorithm can be implemented in about 10 lines of code in any standard computing language; we used Python 3 for the STARS results presented in this paper. 

We witness best performance in STARS when the two hyperparameters the algorithm depends on are close to their close values. We now discuss learning these important hyperparameters algorithmically.


\subsection{Learning Hyperparameters} 
STARS, like many DFO algorithms, exhibits optimal convergence if and only if its hyperparameters -- namely the smoothing factor and step size -- are properly tuned. The smoothing factor and step size for STARS will be precisely defined in the proceeding section; for now, it is enough to state that the hyperparameters depend on the bounded variance in the noise $\sigma^2$ and the $L_1$ Lipschitz constant of the objective function $f$ 
\footnote{i.e., the value $L_1> 0: ||\nabla f(\lambda^1)-\nabla f(\lambda^2)|| \leq L_1 ||\lambda^1 -\lambda^2||$ $\forall \lambda^1, \lambda^2 \in \Lambda$, where $||\cdot||$ denotes a norm on $\Lambda$ (which is a \textit{normed linear space}).}
which are both generally unknown, as well as $\dim \Lambda=P$, which is assumed to be known. 

Tuning the hyperparameters in STARS is a matter of learning $\sigma^2$ and $L_1$. In this paper, we propose algorithmic methods of learning STARS hyperparameters so that one need not specify the hyperparameters at all, fully automating the process of solving optimization problems \eqref{eq:1} and \eqref{eq:2}. Since we are considering $f$ to be a a black-box, it is unrealistic to assume that a user would know $\sigma^2$, much less $L_1$, which would require at least some type of gradient information.

To learn the variance in the noise, we rely on the EC-Noise algorithm [CITE], which even for $P$ large requires few evaluations of $f$ -- often 6 to 10 evaluations will suffice. Briefly, EC-Noise uses a series of nearby samples $s^i:=(\lambda^i,f(\lambda^i))$ of points $\lambda^i$ along line in $\Lambda$. Forming a classical difference table of iterative residuals, the authors show that estimators $\hat{\sigma^2}$ to $\sigma^2$ are formed. Learning $\sigma^2$ is performed prior to STARS, and may be viewed as a computational expense we must pay up front to ensure convergence of the algorithm.

Learning $L_1$ in this setting is a challenge, mainly since we assume that we lack access to $\nabla f$. Given $S$ pointwise estimates to the gradient, which we denote with $\hat{\nabla}f(\lambda^i)$, $i=1,\ldots, S$, we could consider an estimator such as



\begin{equation} \label{eq:3}
\reallywidehat{L_1}:= \max_{i \neq j} \frac{\left|\left|\reallywidehat{\nabla f}(\lambda^{i})-\reallywidehat{\nabla f}(\lambda^{j})\right|\right|-2\epsilon^*}{||\lambda^{i}-\lambda^{j}||}, \quad \epsilon^*=\sup_\xi |\epsilon(\xi)|, \quad i,j=1,\ldots, S,
\end{equation} 

\noindent which is a ``bad idea" given in \cite{Calliess}. There are several problems with such an approach. Mainly, forming $\hat{\nabla} f$ is expensive, requiring at least $P+1$ evaluations of $f$ for each approximation. Even if one uses only 3 samples $s_i$, $i=1,2,3$, forming $\hat{L_1}$ requires $3(P+1)$ $f$ evaluations, which will often exceed the number of function evaluations needed for STARS to converge, assuming its hyperparameters are reasonably tuned.

Another challenge involves specifying $\epsilon^*$ in \eqref{eq:3}, which is subtracted from the estimator's numerator to control for noise in $f$. To save computational expense, we propose forming an initial estimate to $L_1$ by recycling function values from EC-Noise in a finite-difference fashion. Then, once STARS is initiated, $L_1$ can be updated using information from iterates (and their corresponding function values). In fact, the iterates (and intermediate iterates) formed during STARS lend themselves to second-order finite differencing to estimate an upper bound on $||\nabla ^2 f||,$ which is a bound for $L_1$.

Both hyperparmeters must be adjusted if dimension reduction is performed because both of the values depend on $P$. We note that both parameters have an inverse proportionality with $P$. Hence, if $\Lambda$ permits dimension reduction to $k$ \textit{active directions} where, say, $k<<P$, both hyperparemeters will be larger using $k$ for $\dim \Lambda$ instead of $P.$ We now turn to introducing dimension reduction in our setting.





\subsection{Dimension Reduction}
We consider functions from a high-dimensional space $\Lambda$ to a data space $\D$ of smaller dimension; i.e., $\dim \D << \dim \Lambda$. Many functions of interest
actually represent postprocessed quantities from the solution of complex physical models. 
It is not often the case that every parameter has an equal impact on function values; usually some parameters matter more than others. If it is possible to mimic the response of $f$ by processing fewer parameters, we can expect computational savings.

We consider Active Subspace (AS) methods described by Paul Constantine in \cite{Constantine2015} and an equivalent method by T.M. Russi in \cite{Russi}. These techniques seek to explain outputs $f(\Lambda)$ in a subspace $\A \subset \Lambda$ for which the $\dim (\A) <P$ or perhaps $\dim(\A)<<P$. Here we discuss the theoretical formulation of $\A$. The details of finding $\A$ algorithmically is discussed in the proceeding section.

We note that AS requires, at the very least, approximations to $\nabla f$. For the discussion in this section, we continue with the understanding that $\nabla f$ will require approximation in some fashion, the details of which will be discussed in the proceeding Methods section. Here $\nabla f(\lambda)$ is $P\times 1$ containing the $P$ partial derivatives of $f$, which for this discussion we assume exist, and are square integrable in $\Lambda$ equipped with some probability density $\pi_\Lambda(\lambda)$ that is positive everywhere in $\Lambda$ and 0 otherwise.
 
AS -- and many other dimension reduction techniques \cite{Russi} -- one transforms inputs $\lambda$ to the origin with some fixed variance, typically so that $\lambda\in [-1,1]^P$. Then, as in \cite{ConstantineMC}, we write the \textit{sensitivity matrix}

\begin{equation} \label{eq:4}
W=\int_\Lambda \nabla f(\lambda) \nabla f(\lambda)^\top  \pi_\Lambda(\lambda) d\lambda,
\end{equation} 

\noindent which is a $P\times P$ symmetric positive semi-definite matrix defining a certain covariance of $\nabla f$ in $\Lambda$. This interpretation of \eqref{eq:4} leads one to the idea of computing the Singular Value Decomposition (SVD) of $W$,

\begin{equation} \label{eq:5}
W=U\Sigma V^*,
\end{equation} 

\noindent where $U$ is $P \times P$ unitary, $\Sigma$ is $P \times P$ diagonal with the singular values of $W$ along its diagonal, and $V^*$ is $P \times P$ unitary. With the singular values of $W$ in hand from its SVD, we search for a drop-off in the spectrum of $W$. In detail, one may plot the singular values, $\{\sigma_i\}_{i=1}^P$ and seek a drop-off in magnitude between some pair of singular values, $\sigma_{j}$ and $\sigma_{j+1}$, $1\leq j \leq j+1 \leq P$, where $\sigma_{j}>>\sigma_{j+1}$. The active subspace is the span of $v_1,\ldots,v_{j}$, which are the first $j$ columns of $V$, the right singular vectors of $W$. We let

\begin{equation} \label{eq:6}
\A(f)=\text{span}\{v_1,\ldots,v_j\}
\end{equation}

\noindent denote the \textit{active subspace} of $f$.


The fact that $v_1,\ldots,v_{j}$ correspond to the nontrivial singular values is exactly why they account for the most amount of variance in function values. In fact, one can view AS as an artful choice of principal components after a \textit{full} Principal Component Analysis (PCA) is performed in the gradient space $W$; for more details on this viewpoint, we refer the reader to Section 6.4 in \cite{Russi}.

For a point $\lambda \in \Lambda$, we define

\begin{equation} \label{eq:7}
  \mathcal{P}_\A(\lambda)=\sum_{i=1}^{j}\left( v_i^T \lambda\right)v_i \in \A, 
\end{equation}

\noindent which is a projection of the point $\lambda$ in the active subspace of $f$. We call this projection an \textit{active variable}, which is a point in the active subspace $\A$. We have arrived at the property that 

\begin{equation} \label{eq:8}
f\left(\mathcal{P}_\A(\lambda)\right) \approx f(\lambda).
\end{equation}

The property above gives the ability to save computational expense for a number of problems in Uncertainty Quantification, including optimization, representation and solving inverse problems. 

\vspace{1cm}

With the literature review and introduced notation provided above, the remaining sections focus on methodology, results, and conclusions. 

In the proceeding Methods section, we thoroughly discuss the details of the STARS algorithm, algorithmic hyperparameter learning, and Monte Carlo-style AS, which are the key ingredients needed for fully-automated STARS performed only in active directions. We then discuss the techniques used to create the fully-automoted \textit{active STARS} algorithm.

In the Results section, we consider 2 example problems with maps that mimic the behavior of the noisy black boxes we consider. We compare learned hyperparameters to their true values, which are specified by the example functions we choose. We also compare the performance of STARS in full variables to active STARS.

Finally, in the Conclusion and Discussion section, we review the extent to which active STARS is effective in our setting. Limitations are discussed and future research questions are posed. 

\section{Methods}

In the following, we begin by detailing STARS, learning STARS hyperparameters, and learning the active subspace of $f$. Then, we propose a modification to STARS so that hyperparameters and the active subspace is learned using information from iterates


\subsection{STARS}

We first present STARS suited for additive OUU as in \eqref{eq:1} in the  pseudocode below.

\vspace{.25cm}


\begin{algorithm}[H]

\SetAlgoLined

	\KwIn{\texttt{maxit}$=:M$; $\lambda^{(0)}$; $f_0:=f(\lambda^{(0)})$; $h$; $k=1$}
 
    \While{$k\leq M$}{
    
  Form smoothing factor $\mu^*_k$
    
  Draw $u^{(k)}$, where $u^{(k)}_p \sim N(0,1)$ for $p=1,\ldots,P$\;
  
  Evaluate $g_k:=f(\lambda^{(k-1)}+\mu^*_ku^{(k)})$\;
  
  Set $ d^{(k)}:=\frac{g_k-f_{k-1}}{\mu^*_k}u^{(k)}$\;
  
  Set $\lambda^{(k)}=\lambda^{(k-1)}-h\cdot d^{(k)}$\;
  
  Evaluate $f_k:=f(\lambda^{(k)})$\; 
  
  Set $k=k+1$\;}

    \KwOut{($\lambda^{(M)}$, $f_M$)}

	\caption{STARS for Additive or Multiplicative OUU}

\end{algorithm}

\vspace{.25cm}

\noindent Observe that STARS requires defining hyperparameters $\mu^*_k$ and $h$, which are the algorithm's smoothing factor and step size, respectively. The step length $h$ will remain constant for all iterations regardless of whether we consider Additive or Multiplicative OUU. In the case of Additive OUU, $\mu^*_k$ will also be constant, i.e., $\mu^*_k=\mu^*$, a fixed value for all iterations $k=1, \ldots, M$. However in the case of Multiplicative OUU, the smoothing factor $\mu^*_k$ will need to be adjusted for every iteration $k$.  

For the additive noise OUU problem \eqref{eq:1}, the values for $\mu^*$ and $h$ are 

\begin{eqnarray} \label{eq:9}
\mu^*:=\left( \frac{8\sigma^2 P}{L_1^2(P+6)^3}\right)^{1/4} \quad \quad h:=(4L_1(P+4))^{-1},
\end{eqnarray} 

\noindent which are proven as optimal values for the algorithm's convergence in \cite{CW}. 

In the multiplicative noise OUU problem \eqref{eq:2}, the step length $h$ remains the same, exactly as in \eqref{eq:9} above, held constant for each iteration. However the smoothing factor must be updated for each iteration $k=1,\ldots, M.$ As shown in \cite{CW}, the optimal smoothing factor for a given iterate $k$ is given by

\begin{eqnarray} \label{eq:10}
\mu^*_k:=\left( \frac{16 \sigma^2 f(\lambda^{(k)};\xi_k)^2 P}{L_1^2(1+3\sigma^2)(P+6)^3}\right)^{1/4}.
\end{eqnarray} 

\noindent 


\subsection{Hyperparameter Learning}


\subsection{Active Subspace Learning}

In the following, we present a method for computing an active subspace $\A$ in our setting. In practice, finding an active subspace of $f$ will require forming an approximation to $W$ in \eqref{eq:4} in a Monte Carlo fashion \cite{ConstantineMC}. We choose to present a Monte Carlo approach that is simple to implement, found in \cite{Russi}. In short, for a random draw $\lambda^i \in \Lambda$, we will find its evaluation under and approximate gradient formed with a surrogate and store the row vector $\nabla f(\lambda^i)^\top$ in a matrix $\tilde{W}$. In fact, $\tilde{W}$ will have a SVD corresponding to \eqref{eq:5}, up to scaling.

In the following, we assume that we lack an analytic form of $\nabla f$; if the analytic gradient is available, the proceeding Monte Carlo method remains valid, and all notations corresponding to approximating $\nabla f$ may be dropped.


One initializes the method by performing $S$ random draws of $\lambda^i \in \Lambda$. We then compute $f(\lambda^i)$ for all $i=1,\ldots,S$ samples, which we note will require, at the very least, $S$ evaluations of $f$; in a realistic setting, this would require $S$ solves of a model such as a PDE-constrained system. We define $D_S:=\{(\lambda^i,f(\lambda^i))\}_{i=1}^S$, a set of $S$ pairs of samples $\lambda^i$ and their function values. 

Next, we need $\nabla_\Lambda f$ -- which we assume that we do not have in closed analytic form -- evaluated at $\lambda^i$ for all $i=1,\ldots,S$. Hence, we generally need a gradient approximation method \cite{Constantine2015, Smith}. Most often, one forms a local linear, global linear, or global quadratic surrogate to $f$ using $D_S$. One may approximate $\nabla f$ using the gradient of the closed-form surrogate. With this approximation formed, we denote each estimation to $\nabla f(\lambda^i)$ with $\hat{\nabla f}(\lambda^i)$ and we define the $P \times S$ matrix $\tilde{W}$ (which is presented below as $\tilde{W}^\top$)

\begin{equation} \label{eq:10}
\tilde{W}^\top:=\begin{bmatrix}
\reallywidehat{\nabla f}(\lambda^1)
\cdot \cdot \cdot
\reallywidehat{\nabla f}(\lambda^S)\\
\end{bmatrix}.
\end{equation}  

Forming the SVD of $\tilde{W}$, $\tilde{W}=\tilde{U}\tilde{\Sigma}\tilde{V}^*$, we search for a drop off in the magnitude of the singular values $\{\tilde{\sigma}_i\}_{i=1}^S$. Assuming such a drop off occurs for an index $j:1<j<S$, we have the $j$ corresponding right singular vectors,$ \tilde{v}_1,\ldots,\tilde{v}_{j}$.  We let $\A\left(f; D_S \right):=\text{span}\{\tilde{v}_1,\ldots,\tilde{v}_{j}\}$ denote the active subspace of $f$ with respect to the samples $D_S$. We choose to use a notation with $D_S$ included to emphasize the dependence of the active subspace on the random draws made in $\Lambda$, which led to our particular $D_S$ set of samples.

In practice, we can check the extent to which the active subspace accounts for functions values $f(\lambda)$ 
by checking for resolution in a \emph{sufficient summary plot} \cite{Constantine2015}, where we plot active variables against function values. In these plots, we hope to see a pattern between the active variables versus their function values. For example, if $f$ is quadratic in its active variables, then we expect to see quadratic-resolved sufficient summary plots.











\subsection{STARS in the Active Variables Only} Given some $f$ and its corresponding active subspace $\A(f;D_S)$ found by the Monte Carlo method discussed in the preceding section, we are interested in investigating the effectiveness of only optimizing $f$ in its active variables. There are several approaches one may consider, and handful of those approaches and their corresponding results are discussed in the proceeding section. The most compelling approach we have observed is to modify the DFO algorithm discussed above to only take random walks in directions lying in $\A$. That is, at iteration $k$, standard DFO algorithms \cite{CW} use random walks given by drawing a random vector $v^{(k)}$ of dimension $N$ in which every component $v_i^{(k)},i=1,\ldots,N$ of $v$ is  drawn from a specified normal distribution. Instead, given the first $j$ singular unit vectors $u_1,\ldots,u_j$ corresponding to the SVD of $\hat{W}$, one may take $j$ draws from a specified normal distribution, which we denote with $s_i\sim N(\mu,\sigma^2)$, and form the random vector $v$ for the $k$-th step in a DFO algorithm as $v^{(k)}=\sum_{i=1}^js_iu_i$, which is just a linear combination of the active variables of $f$ with coefficients given by random draws; we see that such a vector could be interpreted as a random walk in $\A$.

Relevant research directions include: considering other ways to embed the information from $\A$ within a DFO scheme; analyzing the difference between minimizing $f$ in all its variables versus only in its active subspace; considering ways in which the problem may be solved with the mentioned techniques in tandem, such as using a ``burn-in" phase in which $f$ is minimized over all of $\Lambda$ followed by an active variables minimization.



\section{Results}

In this section, we re-visit the questions from the previous section to discuss preliminary results, if any, and present a plan to begin investigating questions that are unanswered. We present a guiding example which is discussed in below sections in the context of relevant research questions and results.

\vspace{.125cm}

\noindent \textbf{Example 3.} \textit{Let $\Lambda=[-1,1]^{11}$ and define $$f(\lambda)=\sum_{i=0}^{10} 2^{(-1)^i i}\lambda_i^2+\epsilon(\lambda),$$ where $\epsilon(\lambda)$ is a draw of additive noise corresponding to the input $\lambda$; here, we take draws of $\epsilon$ of order $10^{-4}$. We see that $\mathcal{D}=[0,2^{10}]$ and $N=11$, $M=1$. Note that the minimimum of $f$ is given by $0 \in \Lambda$. Here, as $i$ increases, terms in $f$ become either more important or less important, depending on whether $i$ is even or odd.}






\subsection{From model problems to fusion applications}

We propose to investigate the approximate solution of data-consistent inverse problems via optimization approaches.  For derivative free optimization, we plan to transition from our noisy
functions to the data consistent inversion of a subgrid LES model for isotropic turbulent flow.  
While this would certainly be a publishable result, the hope is to investigate approximate DCI
when applied to anomalous diffusion in the axisymmetric gyrokinetic code XGCa.

In the case of models where $\lambda$-gradients are available and not of suspect quality, we will
transition from an approximate DCI for a model elliptic problem with a parameterized forcing function, with parameter gradients obtained via adjoint methods.  Success here will allow us to extend the
method to the determination of data-consistent MHD equilibria (via the Grad-Shafranov equation).  





\subsection{Software Dissemination}

Currently, only some of the software used to produce results in this paper are publicly available on GitHub.com. Some of the algorithms used here and other algorithms that are of interest are within open-source packages available online including those of Constantine and Butler et al. Other schemes considered in this paper make modifications to given algorithms and remain under development. A major goal of this thesis proposal will be developing well-documented, open-source software complete with python Jupyter Notebooks and illustrative, replicable examples.


\section{Conclusion and Discussion}


%%%%%%%%%%%%%% References %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{65}





\bibitem{Calliess} Jan-Peter Calliess. ``Lipschitz optimisation for Lipschitz interpolation." In 2017 American Control Conference (ACC 2017), Seattle, WA, USA, May 2017.

\bibitem{CW} Chen and Wild. ``Randomized Derivative-Free Optimization of Noisy Convex Functions." Funded by the Department of Energy. 2015.

\bibitem{Constantine2015} Constantine, Paul G. ``Active Subspaces: Emerging Ideas for Dimension Reduction in Parameter Studies." SIAM, 2015.

\bibitem{ConstantineMC} Constantine, Eftekhari, Wakin. ``Computing Active Subspaces Efficiently with Gradient Sketching." Conference paper, 2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP).


\bibitem{KS} Kvasov and Sergeyev. ``Lipschitz gradients for global optimization in a one-point-based partitioning scheme." Journal of Computational and Applied Mathematics. Volume 236, Issue 16, pp. 4042-4054. 2012.

\bibitem{lea2000} Lea, Daniel J. and Allen, Myles R. and Haine, Thomas W. N. ``Sensitivity analysis of the climate of a chaotic system." Tellus A, Volume 52, No. 5, pp. 523-532. 2000.


\bibitem{Qiqi2014} Qiqi Wang and Rui Hu and Patrick Blonigan. ``Least Squares Shadowing sensitivity analysis of chaotic limit cycle oscillations." Journal of Computational Physics, Volume 267, pp. 210-224. 2014.

\bibitem{Russi} Russi, Trent M. ``Uncertainty Quantification with Experimental Data and Complex System Models." Dissertation, University of California Berkeley. 2010.

\bibitem{Smith}  Smith, Ralph.``Uncertainty Quantification: Theory, Implementation, and Applications.â€ SIAM, 2013.


\bibitem{Tarantola} Tarantola, Albert. ``Inverse Problem Theory and Methods for Model Parameter Estimation." SIAM. 2005.



\bibitem{wildeyprez} Wildey, T., Butler, T., Jakeman, J., Walsh, S. " A Consistent Bayesian Approach for Stochastic Inverse Problems Based on Push-forward Measures." SAND2017-3436PE. 2017.


\end{thebibliography}


\newpage



$$D_S:=\{(\lambda^i,f(\lambda^i))\}_{i=1}^S$$

\vspace{2cm}

$$\textit{Active subspace} \quad A(f;D_S):=\text{span}\{\Tilde{v}_i\}_{i=1}^j$$

\vspace{2cm}

$$u:=\sum_{i=1}^N\omega_i \Tilde{u}_i \quad \text{or} \quad u_j^{(i)} \sim N(0,\omega_i)$$

\vspace{2cm}

$$f: \mathbb{R}^{40} \to \mathbb{R} \quad \quad  f(\lambda; \xi):=\lambda_{20}^2 + \epsilon(\xi) \quad \quad \epsilon(\cdot) \sim U[-k,k], \quad k=1 \times 10^{-3}.$$

\newpage

$$\Delta^{h,0} f =f \quad \quad \Delta^{h,k+1} f=\Delta^{h,k} f(\lambda+h)- \Delta^{h,k} f(\lambda), \quad 1>>h>0, \quad k \geq 0$$

\vspace{.95cm}

$$\textit{$k$-th level estimate to $\sigma,$}\quad \hat{\sigma_k}^2:=C\sum_{i=1}^{S-k}\left(\Delta^{h,k}f(\lambda_i)\right)^2$$


\vspace{3cm}


$$\reallywidehat{L_1}:= \max_{i \neq j} \frac{\left|\left|\reallywidehat{\nabla f}(\lambda^{(i)})-\reallywidehat{\nabla f}(\lambda^{(j)})\right|\right|-2\epsilon^*}{||\lambda^{(i)}-\lambda^{(j)}||}, \quad \epsilon^*\approx 3\sigma, \quad i,j=1,\ldots, S.$$

\vspace{1cm}

$$
\tilde{W}:=\begin{bmatrix}
\reallywidehat{\nabla f}(\lambda_1)
\cdot \cdot \cdot
\reallywidehat{\nabla f}(\lambda_S)\\
\end{bmatrix}^\top\quad \quad \tilde{W}=\tilde{U}\tilde{\Sigma}\tilde{V}^*$$

\vspace{.5cm}

$$\tilde{\Sigma}_{j,j}^2>>\tilde{\Sigma}_{j+1,j+1}^2, \quad 1\leq j <P$$

\vspace{2cm}

$$\tilde{\Sigma}_{j}^2>>\tilde{\Sigma}_{j+1}^2, \quad 1\leq j <P$$



\end{document}
